{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVsw1sgj0KtO"
      },
      "source": [
        "# Automatic Waste Segregator: CNN for Real-World Application\n",
        "This Notebook assumed pre-knowledge about how CNN works, Numpy, Pandas and how to code a basic CNN for classifying MNIST Numbers. The matplotlib functions have self-explanatory names and do not need much delving into"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuwe59r5hPAQ",
        "outputId": "436fee88-324c-4986-a4df-c105aeeb1999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AWS'...\n",
            "remote: Enumerating objects: 2704, done.\u001b[K\n",
            "remote: Counting objects: 100% (149/149), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 2704 (delta 76), reused 149 (delta 76), pack-reused 2555\u001b[K\n",
            "Receiving objects: 100% (2704/2704), 493.13 MiB | 22.28 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n",
            "Updating files: 100% (2427/2427), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/RishiNandha/AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5mmb3LUhtEP"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Dropout, SpatialDropout2D, AveragePooling2D\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imread, show, imshow\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "os.chdir('AWS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCe3SxAYjQjA"
      },
      "source": [
        "## Dataset Preprocessing\n",
        "This is where we prepare the Training Set and the Validation Set\n",
        "\n",
        "### Challenges in any Real-World CNN Application\n",
        "\n",
        "We have a dataset of 1995 Images. This has two challenges:\n",
        "\n",
        "1. We cannot load the entire data into one numpy array in one shot. This will require the computer to allocate RAM to Python in the order of GBs\n",
        "\n",
        "2. 1995 Datapoints is a very small number to get good accuracies from.\n",
        "\n",
        "### 1. Lazy-Loading\n",
        "\n",
        "The first challenged is solved by \"Lazy-Loading\" the data. That is, the Data isn't loaded into a Numpy Array at the start of the program itself, instead we define a `flow_from_directory()` which takes the images from the given folder only when required.\n",
        "\n",
        "The folder has to have subfolders with names corresponding to the Class of the image, with images inside these subfolders\n",
        "\n",
        "### 2. Data Augmentation\n",
        "\n",
        "The second challenge is mitigate by randomizing transformations on the existing images to form new images. For example, a cat's image slightly rotated is still a cat. This is done using `ImageDataGenerator()`.\n",
        "\n",
        "Note that these transforms should be done only to a sensible extent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8PD4l5fjWi2",
        "outputId": "56dbf4d5-5c58-4d65-bcbd-174f0447fc07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1995 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "batchsize = 32\n",
        "# Data Augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "   \tshear_range=0.1,\theight_shift_range=0.2,\n",
        "        zoom_range=0.2,     horizontal_flip=True,\n",
        "        vertical_flip=True, width_shift_range=0.2)\n",
        "\n",
        "# Lazy Loading\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'train',           target_size=(256, 256),\n",
        "        batch_size=batchsize,      class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuhNXqzDrZSW"
      },
      "source": [
        "### 3. Validation Split\n",
        "Having a Validation Set other than just Training and Testing is important. This is because adding Validation Accuracy as a Metric allows us to track for Underfitting and Overfitting. This also allows us to set an arbitrarily large number of Epochs and \"Checkpoint the Training\" & \"Early Stop the Training\" (Discussed later)\n",
        "\n",
        "**Underfitting** - Both Training accuracy and Validation accuracy saturate to a small value\n",
        "\n",
        "**Overfitting** - Training accuracy gets unrealistically good but the Validation accuracy either stops increasing or starts decreasing\n",
        "\n",
        "There are pre-defined methods to do the Validation Split too but below is a demonstration of how to manually prepare a dataset. The output layer predicts a numpy array of probabilities, so the ideal predictions that we train the model to should be numpy arrays with `1` in the index corresponding to the right class and `0` everywhere else, length matched with the size of the output layer\n",
        "\n",
        "\n",
        "The pre-defined method would be much faster than this ofcourse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HJoVWEcBrX13",
        "outputId": "8c66cdb0-9881-4e81-cd42-6c6aac6757f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(406, 256, 256, 3) \t (406, 6)\n"
          ]
        }
      ],
      "source": [
        "# os.chdir() sets the current folder\n",
        "os.chdir('predb')\n",
        "\n",
        "# Empty Numpy Arrays initialized to start appending data into\n",
        "\n",
        "y_val = np.array([]).reshape((0,6))\n",
        "x_val = np.array([]).reshape((0,256,256,3))\n",
        "\n",
        "# Encoding in the right format to be matched against Output Layer\n",
        "\n",
        "def encode_class(x):\n",
        "\tl = [0 for i in range(6)]\n",
        "\tl[x]=1\n",
        "\treturn np.array(l).reshape((1,6))\n",
        "\n",
        "def load_data(name):\n",
        "\tglobal x_val\n",
        "\tglobal y_val\n",
        "\n",
        "    # x = 5 for \"Mixed\"\n",
        "\n",
        "\tx=5\n",
        "\n",
        "    # Re-sizing Images to match Input Layer\n",
        "\n",
        "\timage = plt.imread(name)\n",
        "\timage = cv2.resize(image,(256,256))\n",
        "\n",
        "\tx_val = np.append(x_val, image.reshape((1,256,256,3)),axis=0)\n",
        "\n",
        "    # In the predb folder, images have been named according to their classes.\n",
        "    # So this is checking if the class names are a substring of filename\n",
        "\n",
        "\tif 'cardboard' in name:\n",
        "\t\tx=0\n",
        "\telif 'glass' in name:\n",
        "\t\tx=1\n",
        "\telif 'metal' in name:\n",
        "\t\tx=2\n",
        "\telif 'paper' in name:\n",
        "\t\tx=3\n",
        "\telif 'plastic' in name:\n",
        "\t\tx=4\n",
        "\n",
        "\ty_val = np.append(y_val, encode_class(x),axis=0)\n",
        "\n",
        "# os.listdir() lists all filenames and folders in the current folder\n",
        "\n",
        "for i in os.listdir()[1:]:\n",
        "\tload_data(i)\n",
        "\n",
        "print(x_val.shape, '\\t', y_val.shape)\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNim3eEIjJHz"
      },
      "source": [
        "## Model Definition\n",
        "This is where the educated guessing of what works goes.\n",
        "\n",
        "**Underfit:** We can either increase the number of layers or increase the size of layers or decrease the learning rate\n",
        "\n",
        "**Overfit:** We can either decrease the number of layers or decrease the size of layers or add dropout layers if its over-fitting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2SEWjTbhiKwX"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "# Rishi's Model\n",
        "layerz = [Conv2D(72, kernel_size=(3,3), input_shape=(256,256,3)),\n",
        "\t\tConv2D(72, kernel_size=(3,3),activation='relu'),\n",
        "\t\tMaxPooling2D(pool_size=(2,2)),\n",
        "\t\tConv2D(72, kernel_size=(3,3)),\n",
        "\t\tConv2D(72, kernel_size=(3,3),activation='relu'),\n",
        "\t\tMaxPooling2D(pool_size=(2,2)),\n",
        "\t\tConv2D(72, kernel_size=(3,3)),\n",
        "\t\tConv2D(72, kernel_size=(3,3),activation='relu'),\n",
        "\t\tMaxPooling2D(pool_size=(2,2)),\n",
        "\t\tFlatten(),\n",
        "\t\tDense(250, activation='relu'),\n",
        "\t\tDense(6,activation='softmax')]\n",
        "\n",
        "# AM's model\n",
        "# layerz = [Conv2D(128,(3,3),input_shape=(256,256,3), activation='relu'),\n",
        "# \t\tMaxPooling2D(pool_size=2),\n",
        "# \t\tConv2D(32,(3,3),activation='relu'),\n",
        "# \t\tMaxPooling2D(pool_size=2),\n",
        "# \t\tFlatten(),\n",
        "# \t\tDense(512,activation='relu'),\n",
        "# \t\tDropout(0.2),\n",
        "# \t\tDense(6,activation='softmax')]\n",
        "\n",
        "# Afeefa's Model\n",
        "# layerz = []\n",
        "\n",
        "model=keras.Sequential(layerz)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0000020*batchsize), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikR0VdXVwb2D"
      },
      "source": [
        "## Training Callbacks\n",
        "The two useful training callbacks we mentioned earlier are \"Checkpoint\" and \"Early Stopping\"\n",
        "\n",
        "### 1. Checkpoint\n",
        "\n",
        "The model can be asked to save the best weights that's been encounter so far into a `hdf5` file. \"Best\" is decided using the validation accuracy.\n",
        "\n",
        "This allows the model to save only up until where the training is still favourable and ignores the changes once over-fitting starts.\n",
        "\n",
        "This also allows to repeatly run the same code to train further using a `try:` statement whenever a `hdf5` file already exists\n",
        "\n",
        "This also allows us to keyboard interrupt training and still have the most recent set of weights\n",
        "\n",
        "### 2. Early Stopping\n",
        "\n",
        "The model can be asked to stop training after waiting for a `patience`-number of epochs. This is done so that the program stops after underfitting or overfitting starts and doesn't waste time running for more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q0M390DfiuzO",
        "outputId": "70b5d553-2027-4a12-d7b7-3790dc820e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Pre-existing Checkpoint, creating new one\n"
          ]
        }
      ],
      "source": [
        "# Checkpoint\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('ckpt.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Early Stopping\n",
        "\n",
        "stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
        "\n",
        "try:\n",
        "    model.load_weights('ckpt.hdf5')\n",
        "except:\n",
        "    print(\"No Pre-existing Checkpoint, creating new one\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTDk1zPUzlwW"
      },
      "source": [
        "## Training\n",
        "\n",
        "Finally, here goes Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi_rAtNbzrIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf326865-d5a8-4dca-ba3a-53cd9831cfab"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 7.0555 - accuracy: 0.3305\n",
            "Epoch 1: val_accuracy improved from -inf to 0.45074, saving model to ckpt.hdf5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110/110 [==============================] - 60s 437ms/step - loss: 7.0555 - accuracy: 0.3305 - val_loss: 1.4420 - val_accuracy: 0.4507\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.4034 - accuracy: 0.4222\n",
            "Epoch 2: val_accuracy did not improve from 0.45074\n",
            "110/110 [==============================] - 33s 302ms/step - loss: 1.4034 - accuracy: 0.4222 - val_loss: 1.3883 - val_accuracy: 0.4507\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.3580 - accuracy: 0.4644\n",
            "Epoch 3: val_accuracy improved from 0.45074 to 0.49507, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 315ms/step - loss: 1.3580 - accuracy: 0.4644 - val_loss: 1.2140 - val_accuracy: 0.4951\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.2776 - accuracy: 0.4917\n",
            "Epoch 4: val_accuracy improved from 0.49507 to 0.53941, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 313ms/step - loss: 1.2776 - accuracy: 0.4917 - val_loss: 1.1958 - val_accuracy: 0.5394\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.2527 - accuracy: 0.5191\n",
            "Epoch 5: val_accuracy improved from 0.53941 to 0.54187, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 312ms/step - loss: 1.2527 - accuracy: 0.5191 - val_loss: 1.1475 - val_accuracy: 0.5419\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.2083 - accuracy: 0.5345\n",
            "Epoch 6: val_accuracy improved from 0.54187 to 0.57882, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 36s 325ms/step - loss: 1.2083 - accuracy: 0.5345 - val_loss: 1.0401 - val_accuracy: 0.5788\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.1829 - accuracy: 0.5556\n",
            "Epoch 7: val_accuracy did not improve from 0.57882\n",
            "110/110 [==============================] - 35s 318ms/step - loss: 1.1829 - accuracy: 0.5556 - val_loss: 1.1763 - val_accuracy: 0.5493\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0976 - accuracy: 0.5801\n",
            "Epoch 8: val_accuracy did not improve from 0.57882\n",
            "110/110 [==============================] - 35s 317ms/step - loss: 1.0976 - accuracy: 0.5801 - val_loss: 1.1312 - val_accuracy: 0.5591\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0770 - accuracy: 0.5926\n",
            "Epoch 9: val_accuracy did not improve from 0.57882\n",
            "110/110 [==============================] - 34s 313ms/step - loss: 1.0770 - accuracy: 0.5926 - val_loss: 1.0639 - val_accuracy: 0.5714\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0972 - accuracy: 0.5795\n",
            "Epoch 10: val_accuracy did not improve from 0.57882\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 1.0972 - accuracy: 0.5795 - val_loss: 1.0773 - val_accuracy: 0.5591\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0448 - accuracy: 0.5926\n",
            "Epoch 11: val_accuracy improved from 0.57882 to 0.61330, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 36s 322ms/step - loss: 1.0448 - accuracy: 0.5926 - val_loss: 0.9953 - val_accuracy: 0.6133\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0511 - accuracy: 0.6137\n",
            "Epoch 12: val_accuracy improved from 0.61330 to 0.63300, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 36s 324ms/step - loss: 1.0511 - accuracy: 0.6137 - val_loss: 0.9681 - val_accuracy: 0.6330\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0176 - accuracy: 0.6097\n",
            "Epoch 13: val_accuracy did not improve from 0.63300\n",
            "110/110 [==============================] - 34s 310ms/step - loss: 1.0176 - accuracy: 0.6097 - val_loss: 0.9873 - val_accuracy: 0.6084\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9701 - accuracy: 0.6251\n",
            "Epoch 14: val_accuracy improved from 0.63300 to 0.64039, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 321ms/step - loss: 0.9701 - accuracy: 0.6251 - val_loss: 0.9592 - val_accuracy: 0.6404\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9930 - accuracy: 0.6336\n",
            "Epoch 15: val_accuracy did not improve from 0.64039\n",
            "110/110 [==============================] - 35s 317ms/step - loss: 0.9930 - accuracy: 0.6336 - val_loss: 1.0306 - val_accuracy: 0.6010\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 1.0187 - accuracy: 0.6199\n",
            "Epoch 16: val_accuracy did not improve from 0.64039\n",
            "110/110 [==============================] - 34s 312ms/step - loss: 1.0187 - accuracy: 0.6199 - val_loss: 0.9810 - val_accuracy: 0.6330\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9423 - accuracy: 0.6479\n",
            "Epoch 17: val_accuracy did not improve from 0.64039\n",
            "110/110 [==============================] - 34s 308ms/step - loss: 0.9423 - accuracy: 0.6479 - val_loss: 1.0683 - val_accuracy: 0.5887\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9742 - accuracy: 0.6290\n",
            "Epoch 18: val_accuracy did not improve from 0.64039\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.9742 - accuracy: 0.6290 - val_loss: 0.9711 - val_accuracy: 0.6232\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9269 - accuracy: 0.6638\n",
            "Epoch 19: val_accuracy did not improve from 0.64039\n",
            "110/110 [==============================] - 34s 313ms/step - loss: 0.9269 - accuracy: 0.6638 - val_loss: 0.9638 - val_accuracy: 0.6330\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8819 - accuracy: 0.6678\n",
            "Epoch 20: val_accuracy improved from 0.64039 to 0.64532, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 37s 332ms/step - loss: 0.8819 - accuracy: 0.6678 - val_loss: 0.9773 - val_accuracy: 0.6453\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9262 - accuracy: 0.6496\n",
            "Epoch 21: val_accuracy did not improve from 0.64532\n",
            "110/110 [==============================] - 34s 312ms/step - loss: 0.9262 - accuracy: 0.6496 - val_loss: 0.9816 - val_accuracy: 0.6207\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.9525 - accuracy: 0.6364\n",
            "Epoch 22: val_accuracy did not improve from 0.64532\n",
            "110/110 [==============================] - 35s 318ms/step - loss: 0.9525 - accuracy: 0.6364 - val_loss: 1.0062 - val_accuracy: 0.6305\n",
            "Epoch 23/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8779 - accuracy: 0.6735\n",
            "Epoch 23: val_accuracy did not improve from 0.64532\n",
            "110/110 [==============================] - 34s 309ms/step - loss: 0.8779 - accuracy: 0.6735 - val_loss: 1.0097 - val_accuracy: 0.6232\n",
            "Epoch 24/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8386 - accuracy: 0.6838\n",
            "Epoch 24: val_accuracy improved from 0.64532 to 0.65271, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 36s 331ms/step - loss: 0.8386 - accuracy: 0.6838 - val_loss: 0.8869 - val_accuracy: 0.6527\n",
            "Epoch 25/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8416 - accuracy: 0.6843\n",
            "Epoch 25: val_accuracy did not improve from 0.65271\n",
            "110/110 [==============================] - 36s 323ms/step - loss: 0.8416 - accuracy: 0.6843 - val_loss: 0.9271 - val_accuracy: 0.6355\n",
            "Epoch 26/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8449 - accuracy: 0.6786\n",
            "Epoch 26: val_accuracy improved from 0.65271 to 0.68227, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.8449 - accuracy: 0.6786 - val_loss: 0.8791 - val_accuracy: 0.6823\n",
            "Epoch 27/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8640 - accuracy: 0.6915\n",
            "Epoch 27: val_accuracy did not improve from 0.68227\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.8640 - accuracy: 0.6915 - val_loss: 0.9169 - val_accuracy: 0.6478\n",
            "Epoch 28/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8101 - accuracy: 0.7060\n",
            "Epoch 28: val_accuracy did not improve from 0.68227\n",
            "110/110 [==============================] - 34s 313ms/step - loss: 0.8101 - accuracy: 0.7060 - val_loss: 0.8456 - val_accuracy: 0.6798\n",
            "Epoch 29/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8580 - accuracy: 0.6781\n",
            "Epoch 29: val_accuracy improved from 0.68227 to 0.70690, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 36s 331ms/step - loss: 0.8580 - accuracy: 0.6781 - val_loss: 0.8104 - val_accuracy: 0.7069\n",
            "Epoch 30/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8250 - accuracy: 0.6989\n",
            "Epoch 30: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 34s 308ms/step - loss: 0.8250 - accuracy: 0.6989 - val_loss: 0.8505 - val_accuracy: 0.6773\n",
            "Epoch 31/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8082 - accuracy: 0.7094\n",
            "Epoch 31: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 36s 325ms/step - loss: 0.8082 - accuracy: 0.7094 - val_loss: 0.8900 - val_accuracy: 0.6650\n",
            "Epoch 32/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.8028 - accuracy: 0.7077\n",
            "Epoch 32: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 34s 306ms/step - loss: 0.8028 - accuracy: 0.7077 - val_loss: 0.9839 - val_accuracy: 0.6502\n",
            "Epoch 33/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7996 - accuracy: 0.7031\n",
            "Epoch 33: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 36s 321ms/step - loss: 0.7996 - accuracy: 0.7031 - val_loss: 0.8376 - val_accuracy: 0.6700\n",
            "Epoch 34/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7560 - accuracy: 0.7185\n",
            "Epoch 34: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 35s 313ms/step - loss: 0.7560 - accuracy: 0.7185 - val_loss: 0.7761 - val_accuracy: 0.7020\n",
            "Epoch 35/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7277 - accuracy: 0.7339\n",
            "Epoch 35: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.7277 - accuracy: 0.7339 - val_loss: 0.8239 - val_accuracy: 0.6872\n",
            "Epoch 36/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7088 - accuracy: 0.7333\n",
            "Epoch 36: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 34s 312ms/step - loss: 0.7088 - accuracy: 0.7333 - val_loss: 0.9222 - val_accuracy: 0.6749\n",
            "Epoch 37/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7730 - accuracy: 0.7100\n",
            "Epoch 37: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 34s 305ms/step - loss: 0.7730 - accuracy: 0.7100 - val_loss: 0.8562 - val_accuracy: 0.6650\n",
            "Epoch 38/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7550 - accuracy: 0.7202\n",
            "Epoch 38: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 34s 311ms/step - loss: 0.7550 - accuracy: 0.7202 - val_loss: 0.7884 - val_accuracy: 0.6995\n",
            "Epoch 39/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7219 - accuracy: 0.7316\n",
            "Epoch 39: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 34s 308ms/step - loss: 0.7219 - accuracy: 0.7316 - val_loss: 0.8621 - val_accuracy: 0.6921\n",
            "Epoch 40/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7338 - accuracy: 0.7373\n",
            "Epoch 40: val_accuracy did not improve from 0.70690\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.7338 - accuracy: 0.7373 - val_loss: 0.8308 - val_accuracy: 0.6946\n",
            "Epoch 41/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7488 - accuracy: 0.7236\n",
            "Epoch 41: val_accuracy improved from 0.70690 to 0.70936, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 322ms/step - loss: 0.7488 - accuracy: 0.7236 - val_loss: 0.8666 - val_accuracy: 0.7094\n",
            "Epoch 42/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.7573\n",
            "Epoch 42: val_accuracy did not improve from 0.70936\n",
            "110/110 [==============================] - 36s 323ms/step - loss: 0.6618 - accuracy: 0.7573 - val_loss: 0.7725 - val_accuracy: 0.7094\n",
            "Epoch 43/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.7544\n",
            "Epoch 43: val_accuracy did not improve from 0.70936\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.6753 - accuracy: 0.7544 - val_loss: 0.8050 - val_accuracy: 0.6921\n",
            "Epoch 44/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6666 - accuracy: 0.7630\n",
            "Epoch 44: val_accuracy improved from 0.70936 to 0.71921, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 315ms/step - loss: 0.6666 - accuracy: 0.7630 - val_loss: 0.7608 - val_accuracy: 0.7192\n",
            "Epoch 45/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.7350\n",
            "Epoch 45: val_accuracy did not improve from 0.71921\n",
            "110/110 [==============================] - 34s 311ms/step - loss: 0.7307 - accuracy: 0.7350 - val_loss: 0.8310 - val_accuracy: 0.6921\n",
            "Epoch 46/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7182 - accuracy: 0.7413\n",
            "Epoch 46: val_accuracy improved from 0.71921 to 0.74384, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 319ms/step - loss: 0.7182 - accuracy: 0.7413 - val_loss: 0.7833 - val_accuracy: 0.7438\n",
            "Epoch 47/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.7550\n",
            "Epoch 47: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 35s 315ms/step - loss: 0.6532 - accuracy: 0.7550 - val_loss: 0.8343 - val_accuracy: 0.6823\n",
            "Epoch 48/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.7630\n",
            "Epoch 48: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 35s 315ms/step - loss: 0.6798 - accuracy: 0.7630 - val_loss: 0.8843 - val_accuracy: 0.7020\n",
            "Epoch 49/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.7618\n",
            "Epoch 49: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.6671 - accuracy: 0.7618 - val_loss: 0.8725 - val_accuracy: 0.7020\n",
            "Epoch 50/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.7732\n",
            "Epoch 50: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 310ms/step - loss: 0.6195 - accuracy: 0.7732 - val_loss: 0.8348 - val_accuracy: 0.7020\n",
            "Epoch 51/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.7818\n",
            "Epoch 51: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 36s 326ms/step - loss: 0.6096 - accuracy: 0.7818 - val_loss: 0.9243 - val_accuracy: 0.6897\n",
            "Epoch 52/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.7705\n",
            "Epoch 52: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 313ms/step - loss: 0.6283 - accuracy: 0.7705 - val_loss: 0.9706 - val_accuracy: 0.6823\n",
            "Epoch 53/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.7010 - accuracy: 0.7504\n",
            "Epoch 53: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 307ms/step - loss: 0.7010 - accuracy: 0.7504 - val_loss: 0.9422 - val_accuracy: 0.6872\n",
            "Epoch 54/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.7783\n",
            "Epoch 54: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 313ms/step - loss: 0.6163 - accuracy: 0.7783 - val_loss: 0.8921 - val_accuracy: 0.6995\n",
            "Epoch 55/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.8023\n",
            "Epoch 55: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 309ms/step - loss: 0.5778 - accuracy: 0.8023 - val_loss: 0.8526 - val_accuracy: 0.7241\n",
            "Epoch 56/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.7647\n",
            "Epoch 56: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 307ms/step - loss: 0.6379 - accuracy: 0.7647 - val_loss: 0.7746 - val_accuracy: 0.7315\n",
            "Epoch 57/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7932\n",
            "Epoch 57: val_accuracy did not improve from 0.74384\n",
            "110/110 [==============================] - 34s 313ms/step - loss: 0.5909 - accuracy: 0.7932 - val_loss: 0.8067 - val_accuracy: 0.7315\n",
            "Epoch 58/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.7909\n",
            "Epoch 58: val_accuracy improved from 0.74384 to 0.75369, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.5961 - accuracy: 0.7909 - val_loss: 0.7749 - val_accuracy: 0.7537\n",
            "Epoch 59/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.7801\n",
            "Epoch 59: val_accuracy improved from 0.75369 to 0.76601, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.5945 - accuracy: 0.7801 - val_loss: 0.7155 - val_accuracy: 0.7660\n",
            "Epoch 60/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5693 - accuracy: 0.7932\n",
            "Epoch 60: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.5693 - accuracy: 0.7932 - val_loss: 0.8022 - val_accuracy: 0.7291\n",
            "Epoch 61/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5556 - accuracy: 0.8006\n",
            "Epoch 61: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 34s 312ms/step - loss: 0.5556 - accuracy: 0.8006 - val_loss: 0.8164 - val_accuracy: 0.7315\n",
            "Epoch 62/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.7920\n",
            "Epoch 62: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 322ms/step - loss: 0.5840 - accuracy: 0.7920 - val_loss: 0.8270 - val_accuracy: 0.7118\n",
            "Epoch 63/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6699 - accuracy: 0.7567\n",
            "Epoch 63: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.6699 - accuracy: 0.7567 - val_loss: 0.8916 - val_accuracy: 0.6921\n",
            "Epoch 64/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5775 - accuracy: 0.7926\n",
            "Epoch 64: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 317ms/step - loss: 0.5775 - accuracy: 0.7926 - val_loss: 0.8567 - val_accuracy: 0.7315\n",
            "Epoch 65/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5561 - accuracy: 0.7903\n",
            "Epoch 65: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 313ms/step - loss: 0.5561 - accuracy: 0.7903 - val_loss: 0.7012 - val_accuracy: 0.7660\n",
            "Epoch 66/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7983\n",
            "Epoch 66: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.5497 - accuracy: 0.7983 - val_loss: 0.7309 - val_accuracy: 0.7660\n",
            "Epoch 67/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.8091\n",
            "Epoch 67: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 34s 308ms/step - loss: 0.5442 - accuracy: 0.8091 - val_loss: 0.9749 - val_accuracy: 0.7291\n",
            "Epoch 68/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.8120\n",
            "Epoch 68: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.5358 - accuracy: 0.8120 - val_loss: 0.7090 - val_accuracy: 0.7463\n",
            "Epoch 69/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.8234\n",
            "Epoch 69: val_accuracy did not improve from 0.76601\n",
            "110/110 [==============================] - 34s 307ms/step - loss: 0.4906 - accuracy: 0.8234 - val_loss: 0.8011 - val_accuracy: 0.7537\n",
            "Epoch 70/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.8046\n",
            "Epoch 70: val_accuracy improved from 0.76601 to 0.77340, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 37s 333ms/step - loss: 0.5497 - accuracy: 0.8046 - val_loss: 0.6861 - val_accuracy: 0.7734\n",
            "Epoch 71/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.8194\n",
            "Epoch 71: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 34s 311ms/step - loss: 0.5187 - accuracy: 0.8194 - val_loss: 0.6786 - val_accuracy: 0.7734\n",
            "Epoch 72/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.8319\n",
            "Epoch 72: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.4727 - accuracy: 0.8319 - val_loss: 0.8112 - val_accuracy: 0.7635\n",
            "Epoch 73/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.8188\n",
            "Epoch 73: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 35s 313ms/step - loss: 0.5318 - accuracy: 0.8188 - val_loss: 0.7916 - val_accuracy: 0.7389\n",
            "Epoch 74/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.8080\n",
            "Epoch 74: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.4971 - accuracy: 0.8080 - val_loss: 0.7780 - val_accuracy: 0.7586\n",
            "Epoch 75/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.8171\n",
            "Epoch 75: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 36s 323ms/step - loss: 0.5064 - accuracy: 0.8171 - val_loss: 0.8953 - val_accuracy: 0.7463\n",
            "Epoch 76/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.8205\n",
            "Epoch 76: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.5057 - accuracy: 0.8205 - val_loss: 0.9832 - val_accuracy: 0.7094\n",
            "Epoch 77/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.8142\n",
            "Epoch 77: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 34s 308ms/step - loss: 0.5104 - accuracy: 0.8142 - val_loss: 0.7883 - val_accuracy: 0.7463\n",
            "Epoch 78/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8217\n",
            "Epoch 78: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.5020 - accuracy: 0.8217 - val_loss: 0.7773 - val_accuracy: 0.7291\n",
            "Epoch 79/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8285\n",
            "Epoch 79: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 36s 324ms/step - loss: 0.5084 - accuracy: 0.8285 - val_loss: 0.7590 - val_accuracy: 0.7611\n",
            "Epoch 80/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.8120\n",
            "Epoch 80: val_accuracy did not improve from 0.77340\n",
            "110/110 [==============================] - 34s 309ms/step - loss: 0.5132 - accuracy: 0.8120 - val_loss: 0.7887 - val_accuracy: 0.7463\n",
            "Epoch 81/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.8325\n",
            "Epoch 81: val_accuracy improved from 0.77340 to 0.78325, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 320ms/step - loss: 0.4727 - accuracy: 0.8325 - val_loss: 0.6679 - val_accuracy: 0.7833\n",
            "Epoch 82/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.8405\n",
            "Epoch 82: val_accuracy did not improve from 0.78325\n",
            "110/110 [==============================] - 34s 307ms/step - loss: 0.4708 - accuracy: 0.8405 - val_loss: 0.7385 - val_accuracy: 0.7783\n",
            "Epoch 83/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.8308\n",
            "Epoch 83: val_accuracy did not improve from 0.78325\n",
            "110/110 [==============================] - 35s 313ms/step - loss: 0.4706 - accuracy: 0.8308 - val_loss: 0.7329 - val_accuracy: 0.7562\n",
            "Epoch 84/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8416\n",
            "Epoch 84: val_accuracy did not improve from 0.78325\n",
            "110/110 [==============================] - 35s 316ms/step - loss: 0.4493 - accuracy: 0.8416 - val_loss: 0.8177 - val_accuracy: 0.7340\n",
            "Epoch 85/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.8398\n",
            "Epoch 85: val_accuracy improved from 0.78325 to 0.78818, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 37s 333ms/step - loss: 0.4638 - accuracy: 0.8398 - val_loss: 0.7785 - val_accuracy: 0.7882\n",
            "Epoch 86/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.8456\n",
            "Epoch 86: val_accuracy did not improve from 0.78818\n",
            "110/110 [==============================] - 35s 318ms/step - loss: 0.4495 - accuracy: 0.8456 - val_loss: 0.7076 - val_accuracy: 0.7734\n",
            "Epoch 87/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.8335\n",
            "Epoch 87: val_accuracy improved from 0.78818 to 0.79310, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 35s 315ms/step - loss: 0.4413 - accuracy: 0.8335 - val_loss: 0.7806 - val_accuracy: 0.7931\n",
            "Epoch 88/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.8336\n",
            "Epoch 88: val_accuracy improved from 0.79310 to 0.79803, saving model to ckpt.hdf5\n",
            "110/110 [==============================] - 36s 328ms/step - loss: 0.4634 - accuracy: 0.8336 - val_loss: 0.6612 - val_accuracy: 0.7980\n",
            "Epoch 89/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.8330\n",
            "Epoch 89: val_accuracy did not improve from 0.79803\n",
            "110/110 [==============================] - 35s 314ms/step - loss: 0.4519 - accuracy: 0.8330 - val_loss: 0.9121 - val_accuracy: 0.7611\n",
            "Epoch 90/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4498 - accuracy: 0.8422\n",
            "Epoch 90: val_accuracy did not improve from 0.79803\n",
            "110/110 [==============================] - 35s 320ms/step - loss: 0.4498 - accuracy: 0.8422 - val_loss: 0.6813 - val_accuracy: 0.7931\n",
            "Epoch 91/100\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.8484\n",
            "Epoch 91: val_accuracy did not improve from 0.79803\n",
            "110/110 [==============================] - 35s 320ms/step - loss: 0.4346 - accuracy: 0.8484 - val_loss: 0.6840 - val_accuracy: 0.7783\n",
            "Epoch 92/100\n",
            " 20/110 [====>.........................] - ETA: 26s - loss: 0.3201 - accuracy: 0.8667"
          ]
        }
      ],
      "source": [
        "model.fit(\n",
        "        train_generator, steps_per_epoch=1995//batchsize,\n",
        "        epochs=100, validation_data=(x_val,y_val), callbacks=[checkpoint, stop])\n",
        "\n",
        "model.save(\"AWS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5HnAZ7z03k6"
      },
      "source": [
        "## Showtime: Testing\n",
        "Here's a loop that picks an image, shows what it is and then runs the model on it. `model(img)` just sends the `img` through the model as if `model()` was a function and returns the output layer's values as a numpy array. Rest of the code is just formatting it for displaying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXNd-TKP09od"
      },
      "outputs": [],
      "source": [
        "l=[\"Cardboard\", \"Glass\", \"Metal\", \"Paper\", \"Plastic\", \"Mixed\"]\n",
        "\n",
        "flag = 1\n",
        "os.chdir('predb')\n",
        "dirlist=os.listdir()\n",
        "print(' ')\n",
        "while flag==1:\n",
        "\n",
        "\t# Open Image and Directory\n",
        "\timgdir=dirlist[int(input(\"\\nEnter Test Image Index (Enter an Integer lesser than \"+str(len(dirlist))+\") : \"))]\n",
        "\timg = imread(imgdir)\n",
        "\n",
        "\t# Display Image\n",
        "\timshow(img)\n",
        "\tshow()\n",
        "\n",
        "\t# Run Image Through CNN\n",
        "\timg = resize(img,(256,256))\n",
        "\timg = img.reshape((1,256,256,3))\n",
        "\timg = model.predict(img,verbose=0)\n",
        "\timg=img.reshape((6,))\n",
        "\n",
        "\t# Correct Class\n",
        "\tprint(\"\\nCorrect Class :\", imgdir)\n",
        "\n",
        "\t# Prediction by CNN\n",
        "\tif np.amax(img)<(2.0/3):\n",
        "\t\tprint(\"\\nPredicted Class : Mixed\")\n",
        "\t\tprint(\"Confidence was too low, hence it's precautiously categorized under Mixed\")\n",
        "\telse:\n",
        "\t\tprint(\"\\nPredicted Class :\",l[int(np.where(img == np.amax(img))[0][0])])\n",
        "\t\tprint(\"Confidence :\",(np.amax(img)*100)//1, \"%\")\n",
        "\n",
        "\tflag=int(input(\"\\n\\nContinue? (1/0) : \"))\n",
        "\n",
        "\tprint(\"\\n----------------------------------------------------------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}